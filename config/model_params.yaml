# Hyperparameters for Each Model

# Decision Tree Classifier (NEW)
decision_tree:
  criterion: "gini"  # gini or entropy
  max_depth: 10
  min_samples_split: 5
  min_samples_leaf: 2
  max_features: "sqrt"
  class_weight: "balanced"
  random_state: 42

# Random Forest Classifier
random_forest:
  n_estimators: 200
  max_depth: 15
  min_samples_split: 5
  min_samples_leaf: 2
  max_features: "sqrt"
  bootstrap: true
  class_weight: "balanced"
  random_state: 42
  n_jobs: -1
  verbose: 0

# XGBoost Classifier
xgboost:
  n_estimators: 200
  max_depth: 6
  learning_rate: 0.1
  subsample: 0.8
  colsample_bytree: 0.8
  min_child_weight: 1
  gamma: 0
  reg_alpha: 0
  reg_lambda: 1
  random_state: 42
  n_jobs: -1
  verbosity: 0
  eval_metric: "mlogloss"

# Logistic Regression Classifier
logistic_regression:
  penalty: "l2"
  C: 1.0
  solver: "lbfgs"
  max_iter: 1000
  # multi_class: "multinomial"
  class_weight: "balanced"
  random_state: 42
  n_jobs: -1
  verbose: 0

# Hyperparameter Tuning Grids (Optional)
hyperparameter_grids:
  decision_tree:
    max_depth: [5, 10, 15, 20]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    criterion: ["gini", "entropy"]
  
  random_forest:
    n_estimators: [100, 200, 300]
    max_depth: [10, 15, 20, null]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
  
  xgboost:
    n_estimators: [100, 200, 300]
    max_depth: [3, 6, 9]
    learning_rate: [0.01, 0.1, 0.3]
    subsample: [0.7, 0.8, 0.9]
  
  logistic_regression:
    C: [0.01, 0.1, 1.0, 10.0]
    penalty: ["l2"]
    solver: ["lbfgs", "saga"]